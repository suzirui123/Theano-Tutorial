{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from logistic_sgd.logistic_sgd import LogisticRegression, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...loading data\n",
      "... building the model\n",
      "... training\n",
      "epoch 1, minibatch 2500/2500, validation error 10.090000 %\n",
      "epoch 1, minibatch 2500/2500, test error of best model 9.620000 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-ee89b93d0207>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mtest_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-ee89b93d0207>\u001b[0m in \u001b[0;36mtest_mlp\u001b[0;34m(learning_rate, L1_reg, L2_reg, n_epochs, dataset, batch_size, n_hidden)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mminibatch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mminibatch_avg_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_train_batches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mminibatch_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalidation_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, input, n_in, n_out, W = None, b = None, activation = T.tanh):\n",
    "        self.input = input\n",
    "        if W is None:\n",
    "            W_values = np.asarray(rng.uniform(low = - np.sqrt(6. / (n_in + n_out)), high = np.sqrt(6. / (n_in + n_out)), size = (n_in, n_out)), dtype = theano.config.floatX)\n",
    "            if activation == T.nnet.sigmoid:\n",
    "                W_values *= 4\n",
    "            W = theano.shared(value=W_values, name = 'W', borrow = True)\n",
    "        if b is None:\n",
    "            b_values = np.zeros((n_out, ), dtype=theano.config.floatX)\n",
    "            b = theano.shared(value=b_values, name='b', borrow = True)\n",
    "        \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        lin_output = T.dot(input, self.W) + self.b\n",
    "        self.output = (lin_output if activation is None else activation(lin_output))\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
    "        self.hiddenLayer = HiddenLayer(rng = rng, input = input, n_in = n_in, n_out = n_hidden, activation = T.tanh)\n",
    "        self.logRegressionLayer = LogisticRegression(input=self.hiddenLayer.output, n_in=n_hidden, n_out=n_out)\n",
    "        self.L1 = (abs(self.hiddenLayer.W).sum() + abs(self.logRegressionLayer.W).sum())\n",
    "        self.L2_sqr = ((self.hiddenLayer.W ** 2).sum() + (self.logRegressionLayer.W ** 2).sum())\n",
    "        self.negative_log_likelihood = (self.logRegressionLayer.negative_log_likelihood)\n",
    "        self.errors = self.logRegressionLayer.errors\n",
    "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params\n",
    "\n",
    "def test_mlp(learning_rate = 0.01, L1_reg = 0.00, L2_reg = 0.0001, n_epochs = 1000, dataset = 'mnist.pkl.gz',batch_size = 20, n_hidden = 500):\n",
    "    datasets = load_data(dataset)\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "    \n",
    "    n_train_batches = train_set_x.get_value(borrow = True).shape[0] / batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow = True).shape[0] / batch_size\n",
    "    n_test_batches = test_set_x.get_value(borrow = True).shape[0] / batch_size\n",
    "    \n",
    "    print '... building the model'\n",
    "    index = T.lscalar()\n",
    "    x = T.matrix('x')\n",
    "    y = T.ivector('y')\n",
    "    rng = np.random.RandomState(1234)\n",
    "    classifier = MLP(rng = rng, input = x, n_in = 28 * 28, n_hidden = n_hidden, n_out = 10)\n",
    "    cost = (classifier.negative_log_likelihood(y) + L1_reg * classifier.L1 + L2_reg * classifier.L2_sqr)\n",
    "    test_model = theano.function(inputs = [index],outputs = classifier.errors(y),givens = {x: test_set_x[index * batch_size: (index + 1) * batch_size],y: test_set_y[index * batch_size : (index + 1) * batch_size]})\n",
    "    validate_model = theano.function(inputs = [index], outputs = classifier.errors(y),givens = {x: valid_set_x[index * batch_size: (index + 1) * batch_size],y: valid_set_y[index * batch_size: (index + 1) * batch_size]})\n",
    "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "    updates = [(param, param - learning_rate * gparam) for param, gparam in zip(classifier.params, gparams)]\n",
    "    train_model = theano.function(inputs=[index], outputs=cost, updates=updates,givens={x: train_set_x[index * batch_size: (index + 1) * batch_size],y: train_set_y[index * batch_size: (index + 1) * batch_size]})\n",
    "    print '... training'\n",
    "    patience = 10000\n",
    "    patience_increase = 2\n",
    "    improvement_threshold = 0.995\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "    best_validation_loss = np.inf\n",
    "    best_iter = 0\n",
    "    best_score = 0.\n",
    "    tic = time.clock()\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch += 1\n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                validation_losses = [validate_model(i) for i in xrange(n_valid_batches)]\n",
    "                this_validation_loss = np.mean(validation_losses)\n",
    "                print 'epoch %i, minibatch %i/%i, validation error %f %%' % (epoch, minibatch_index + 1, n_train_batches,this_validation_loss * 100)\n",
    "                if this_validation_loss < best_validation_loss :\n",
    "                    if (this_validation_loss < best_validation_loss * improvement_threshold):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "                    test_losses = [test_model(i) for i in xrange(n_test_batches)]\n",
    "                    test_score = np.mean(test_losses)\n",
    "                    print 'epoch %i, minibatch %i/%i, test error of best model %f %%' % (epoch, minibatch_index + 1, n_train_batches, test_score * 100)\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "    toc = time.clock()\n",
    "    print (('Optimization complete. Best validation score of %f %% obtained at iteration %i, with test performance %f %%') %\n",
    "              (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
